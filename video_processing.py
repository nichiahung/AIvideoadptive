"""
AdaptVideo å½±ç‰‡è™•ç†æ¨¡çµ„
"""
import os
import cv2
import numpy as np
import base64
import json
import uuid
import shutil
import time
from io import BytesIO
from PIL import Image
import httpx
from openai import OpenAI
from config import config

# åˆå§‹åŒ– OpenAI ç”¨æˆ¶ç«¯
try:
    api_key = config.OPENAI_API_KEY
    if not api_key:
        raise ValueError("åœ¨ .env æ–‡ä»¶æˆ–ç’°å¢ƒè®Šæ•¸ä¸­æ‰¾ä¸åˆ° OPENAI_API_KEYã€‚")
    http_client = httpx.Client(proxies={})
    client = OpenAI(api_key=api_key, http_client=http_client)
    LLM_AI_AVAILABLE = True
    print("âœ… OpenAI ç”¨æˆ¶ç«¯å·²æˆåŠŸåˆå§‹åŒ–ã€‚")
except Exception as e:
    client = None
    LLM_AI_AVAILABLE = False
    print(f"âš ï¸ ç„¡æ³•åˆå§‹åŒ– OpenAI ç”¨æˆ¶ç«¯: {e}")

# åŠ è¼‰ OpenCV äººè‡‰åµæ¸¬æ¨¡å‹
try:
    face_cascade_path = cv2.data.haarcascades + 'haarcascade_frontalface_default.xml'
    face_cascade = cv2.CascadeClassifier(face_cascade_path)
    if face_cascade.empty():
        raise IOError(f"ç„¡æ³•å¾è·¯å¾‘åŠ è¼‰ haarcascade: {face_cascade_path}")
    OPENCV_AI_AVAILABLE = True
    print("âœ… OpenCV äººè‡‰åµæ¸¬æ¨¡å‹å·²è¼‰å…¥")
except Exception as e:
    OPENCV_AI_AVAILABLE = False
    print(f"âš ï¸ ç„¡æ³•è¼‰å…¥OpenCVäººè‡‰åµæ¸¬æ¨¡å‹: {e}")

# å˜—è©¦å°å…¥ MoviePy
try:
    from moviepy.editor import VideoFileClip
    MOVIEPY_AVAILABLE = True
    print("âœ… MoviePy å·²è¼‰å…¥ï¼Œæ”¯æ´å®Œæ•´å½±ç‰‡è½‰æ›åŠŸèƒ½")
except ImportError:
    MOVIEPY_AVAILABLE = False
    print("âš ï¸ MoviePy æœªå®‰è£ï¼Œå°‡ä½¿ç”¨åŸºæœ¬åŠŸèƒ½")

def get_video_info(file_path):
    """ç²å–å½±ç‰‡åŸºæœ¬ä¿¡æ¯"""
    try:
        with VideoFileClip(file_path) as clip:
            return {
                "duration": round(clip.duration, 2),
                "width": clip.w,
                "height": clip.h,
                "fps": clip.fps
            }
    except Exception as e:
        print(f"ç²å–å½±ç‰‡ä¿¡æ¯å¤±æ•—: {e}")
        return {"duration": 0, "width": 0, "height": 0, "fps": 0}

def extract_thumbnail(video_path):
    """å¾å½±ç‰‡ä¸­é–“æå–ä¸€å¹€ä½œç‚ºç¸®åœ–"""
    try:
        cap = cv2.VideoCapture(video_path)
        if not cap.isOpened():
            return None
        
        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        cap.set(cv2.CAP_PROP_POS_FRAMES, total_frames // 2)
        ret, frame = cap.read()
        cap.release()
        
        if not ret:
            return None
        
        _, buffer = cv2.imencode(".jpg", frame, [cv2.IMWRITE_JPEG_QUALITY, config.DEFAULT_JPEG_QUALITY])
        return f"data:image/jpeg;base64,{base64.b64encode(buffer).decode('utf-8')}"
    except Exception as e:
        print(f"âŒ æå–ç¸®åœ–å¤±æ•—: {e}")
        return None

def analyze_video_for_face_crop(video_path):
    """åˆ†æå½±ç‰‡ï¼Œæ‰¾åˆ°ä¸»è¦äººè‡‰çš„å¹³å‡ä¸­å¿ƒä½ç½®"""
    if not OPENCV_AI_AVAILABLE:
        return None
    
    cap = cv2.VideoCapture(video_path)
    if not cap.isOpened():
        return None

    face_positions = []
    max_frames_to_check = config.MAX_FRAMES_FOR_ANALYSIS
    frame_count = 0
    
    while cap.isOpened() and frame_count < max_frames_to_check:
        ret, frame = cap.read()
        if not ret:
            break
        
        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
        faces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))
        
        if len(faces) > 0:
            main_face = max(faces, key=lambda r: r[2] * r[3])
            face_positions.append((main_face[0] + main_face[2] / 2, main_face[1] + main_face[3] / 2))
        
        frame_count += 1
    
    cap.release()

    if not face_positions:
        print("â„¹ï¸ åœ¨å½±ç‰‡ä¸­æœªåµæ¸¬åˆ°äººè‡‰")
        return None
    
    avg_pos = np.mean(face_positions, axis=0)
    print(f"âœ… AIåˆ†æå®Œæˆï¼Œå¹³å‡äººè‡‰ä¸­å¿ƒ: ({avg_pos[0]:.0f}, {avg_pos[1]:.0f})")
    return avg_pos

def extract_frames_generic(video_path, num_frames, return_pil=False, max_width=None, quality=85):
    """é€šç”¨çš„å¹€æå–å‡½æ•¸ï¼Œå¯è¿”å› base64 æˆ– PIL Image"""
    cap = cv2.VideoCapture(video_path)
    if not cap.isOpened():
        return []
    
    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
    if total_frames <= 0:
        cap.release()
        return []
    
    # è¨ˆç®—è¦æå–çš„å¹€ç´¢å¼•
    frame_indices = []
    if num_frames > 1 and total_frames > 1:
        for i in range(num_frames):
            frame_idx = int(i * (total_frames - 1) / (num_frames - 1))
            frame_indices.append(min(frame_idx, total_frames - 1))
    else:
        frame_indices = [0]
    
    frames = []
    for frame_idx in frame_indices:
        cap.set(cv2.CAP_PROP_POS_FRAMES, frame_idx)
        ret, frame = cap.read()
        if not ret:
            continue
        
        if return_pil:
            # è½‰æ›ç‚º PIL Image
            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            img = Image.fromarray(frame_rgb)
            
            # å¦‚æœæŒ‡å®šæœ€å¤§å¯¬åº¦ï¼Œèª¿æ•´å¤§å°
            if max_width and img.width > max_width:
                scale = max_width / img.width
                new_width = int(img.width * scale)
                new_height = int(img.height * scale)
                img = img.resize((new_width, new_height), Image.LANCZOS)
            
            frames.append(img)
        else:
            # è¿”å› base64
            _, buffer = cv2.imencode(".jpg", frame, [cv2.IMWRITE_JPEG_QUALITY, quality])
            frames.append(base64.b64encode(buffer).decode("utf-8"))
    
    cap.release()
    return frames

def apply_smart_crop(image, target_width, target_height, center, original_width=None, original_height=None):
    """æ‡‰ç”¨æ™ºæ…§è£åˆ‡é‚è¼¯ï¼Œè¿”å›è£åˆ‡å¾Œçš„åœ–åƒå’Œæ˜¯å¦è¢«èª¿æ•´çš„æ¨™è¨˜"""
    if isinstance(image, np.ndarray):
        # OpenCV æ ¼å¼è½‰ PIL
        image = Image.fromarray(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))
    
    # ç²å–åŸå§‹å°ºå¯¸
    if original_width is None:
        original_width = image.width
    if original_height is None:
        original_height = image.height
    
    # è¨ˆç®—ç¸®æ”¾æ¯”ä¾‹
    scale = max(target_width / original_width, target_height / original_height)
    resized_w = int(original_width * scale)
    resized_h = int(original_height * scale)
    
    # ç¸®æ”¾åœ–åƒ
    resized_img = image.resize((resized_w, resized_h), Image.LANCZOS)
    
    # è¨ˆç®—è£åˆ‡ä¸­å¿ƒé»
    desired_center_x = center[0] * scale
    desired_center_y = center[1] * scale
    
    # è¨ˆç®—å®‰å…¨ç¯„åœ
    half_w = target_width / 2
    half_h = target_height / 2
    min_x = half_w
    max_x = resized_w - half_w
    min_y = half_h
    max_y = resized_h - half_h
    
    # ç¢ºä¿ä¸­å¿ƒé»åœ¨å®‰å…¨ç¯„åœå…§
    final_crop_x = max(min_x, min(desired_center_x, max_x))
    final_crop_y = max(min_y, min(desired_center_y, max_y))
    
    # æª¢æŸ¥æ˜¯å¦è¢«èª¿æ•´
    is_adjusted = abs(final_crop_x - desired_center_x) > 1 or abs(final_crop_y - desired_center_y) > 1
    
    # é€²è¡Œè£åˆ‡
    left = int(final_crop_x - half_w)
    top = int(final_crop_y - half_h)
    right = int(final_crop_x + half_w)
    bottom = int(final_crop_y + half_h)
    
    cropped_img = resized_img.crop((left, top, right, bottom))
    
    return cropped_img, is_adjusted

def analyze_video_with_llm(video_path, conversation_history, original_width=None, original_height=None):
    """ä½¿ç”¨å¤šæ¨¡æ…‹LLMåˆ†æå½±ç‰‡ï¼ŒåŸºæ–¼æä¾›çš„å°è©±æ­·å²"""
    if not LLM_AI_AVAILABLE:
        return None
    
    base64_frames = extract_frames_from_video(video_path, max_frames=5)
    if not base64_frames:
        print("âŒ ç„¡æ³•æå–å¹€é€²è¡ŒLLMåˆ†æ")
        return None
        
    print(f"ğŸ§  å·²æå– {len(base64_frames)} å¹€ï¼Œæº–å‚™åŸºæ–¼å°è©±æ­·å²é€²è¡ŒLLMåˆ†æ...")

    template_descriptions = "\\n".join([f"- {t['name']}: {t['width']}x{t['height']} ({t['description']})" for t in config.DOOH_TEMPLATES])
    
    # å¦‚æœæœ‰æä¾›åŸå§‹å½±ç‰‡å°ºå¯¸ï¼Œå‰‡åŠ å…¥åˆ°æç¤ºä¸­
    original_video_info = ""
    if original_width and original_height:
        original_video_info = f"**ç”¨æˆ¶çš„åŸå§‹å½±ç‰‡å°ºå¯¸ç‚º {original_width}x{original_height}ã€‚**"
    
    # æ§‹å»ºå‚³é€çµ¦OpenAIçš„messages
    messages = [
        {
            "role": "system",
            "content": f"""
ä½ æ˜¯ä¸€ä½å°ˆæ¥­çš„**æ•¸ä½æˆ¶å¤–å»£å‘Š (DOOH) ç‰ˆä½ç­–ç•¥é¡§å•**ã€‚ä½ çš„ä»»å‹™æ˜¯åˆ†æç”¨æˆ¶æä¾›çš„å½±ç‰‡ï¼Œä¸¦æ ¹æ“šå…¶**åŸç”Ÿç‰¹æ€§ï¼ˆå…§å®¹ã€é¢¨æ ¼ã€åŸå§‹å°ºå¯¸ï¼‰**ï¼Œæ¨è–¦æœ€æœ‰æ•ˆçš„ DOOH å»£å‘Šç‰ˆä½ã€‚

{original_video_info}

**ä½ çš„æ ¸å¿ƒè·è²¬:**
1.  **åˆ†æå½±ç‰‡åŸç”Ÿç‰¹æ€§**: è©•ä¼°å½±ç‰‡çš„åŸå§‹å°ºå¯¸ã€é•·å¯¬æ¯”ã€å…§å®¹é¢¨æ ¼ï¼ˆä¾‹å¦‚ï¼šå¿«ç¯€å¥ã€éœæ…‹ã€è¦–è¦ºç„¦é»æ˜¯å¦é›†ä¸­ï¼‰ï¼Œä»¥åŠæ ¸å¿ƒè¡ŒéŠ·è¨Šæ¯ã€‚
2.  **å„ªå…ˆæ¨è–¦ç„¡æç‰ˆä½**: æ ¹æ“šå½±ç‰‡çš„**åŸå§‹é•·å¯¬æ¯”**ï¼Œå¾ä¸‹æ–¹çš„å¯ç”¨å°ºå¯¸æ¨¡æ¿ä¸­æ‰¾å‡º**æœ€ç›¸è¿‘æˆ–å®Œå…¨ç›¸ç¬¦**çš„ç‰ˆä½ã€‚ä½ æ‡‰è©²å„ªå…ˆæ¨è–¦é€™äº›å¯ä»¥**ç„¡éœ€è£åˆ‡ã€ç›´æ¥æŠ•æ”¾**çš„ç‰ˆä½ã€‚åœ¨æ¨è–¦æ™‚ï¼Œå¿…é ˆèªªæ˜ç‚ºä»€éº¼é€™äº› DOOH å ´æ™¯ï¼ˆä¾‹å¦‚ï¼šæ©Ÿå ´çš„è¶…å¯¬è¢å¹•ã€ç™¾è²¨å…¬å¸çš„ç›´å¼è¢å¹•ï¼‰ç‰¹åˆ¥é©åˆé€™æ”¯å½±ç‰‡çš„é¢¨æ ¼èˆ‡å…§å®¹ã€‚
3.  **æä¾›è½‰è£½è£åˆ‡å»ºè­° (æ¬¡è¦)**: å¦‚æœä½ èªç‚ºé€é**æ™ºæ…§è£åˆ‡**å¯ä»¥è®“å½±ç‰‡åœ¨æ›´å¤šä¸»æµã€ä½†é•·å¯¬æ¯”ä¸åŒçš„ DOOH ç‰ˆä½ä¸Šå–å¾—æ›´å¥½çš„æ•ˆæœï¼Œä½ å¯ä»¥**é¡å¤–**å»ºè­° 1-2 å€‹éœ€è¦è½‰æ›çš„å°ºå¯¸ã€‚ç•¶ä½ æå‡ºé€™é¡å»ºè­°æ™‚ï¼Œå¿…é ˆï¼š
    *   æ˜ç¢ºå‘ŠçŸ¥ç”¨æˆ¶é€™éœ€è¦å°å½±ç‰‡é€²è¡Œ**äºŒæ¬¡è™•ç†**ã€‚
    *   è§£é‡‹ç‚ºä»€éº¼é€™æ¨£çš„è£åˆ‡æ˜¯å€¼å¾—çš„ï¼ˆä¾‹å¦‚ï¼šæ›´èƒ½èšç„¦åœ¨ç”¢å“ä¸Šã€æ›´ç¬¦åˆç‰¹å®šå ´æ™¯çš„è¦–è¦ºéœ‡æ’¼åŠ›ï¼‰ã€‚
4.  **è­˜åˆ¥è¡ŒéŠ·ä¸»é«”**: åœ¨å½±ç‰‡ç•«é¢ä¸­æ‰¾å‡ºæœ€é‡è¦çš„è¡ŒéŠ·ä¸»é«”ï¼ˆä¾‹å¦‚ï¼šç”¢å“ã€Logoã€é—œéµäººç‰©ã€æ¨™èªï¼‰ï¼Œé€™å°‡ä½œç‚ºä½ æå‡ºæ™ºæ…§è£åˆ‡å»ºè­°æ™‚çš„ä¾æ“šã€‚
5.  **é‡è¦é™åˆ¶**: ä½ çš„æ¨è–¦æ‡‰åš´æ ¼é™åˆ¶åœ¨ DOOH æ‡‰ç”¨å ´æ™¯ï¼Œä¸¦ä¸»å‹•é¿å…æå‡ºä»»ä½•ç¤¾ç¾¤åª’é«”å¹³å°ï¼ˆå¦‚ Instagram, TikTok, YouTubeï¼‰çš„å»ºè­°ã€‚
6.  **æ ¼å¼åŒ–è¼¸å‡º**: ä½ çš„å›æ‡‰å¿…é ˆæ˜¯æ ¼å¼å®Œæ•´çš„ JSON ç‰©ä»¶ã€‚

**å¯ç”¨å°ºå¯¸æ¨¡æ¿:**
{template_descriptions}

**JSON è¼¸å‡ºæ ¼å¼ç¯„ä¾‹ (å°ˆæ³¨æ–¼ DOOH):**
{{
  "suggestions": "### AI å°ˆæ¥­å»ºè­°\\n\\næ‚¨çš„å½±ç‰‡é¢¨æ ¼å‹•æ„Ÿåè¶³ï¼Œéå¸¸é©åˆæˆ¶å¤–å¤§å‹è¢å¹•ã€‚æ ¹æ“šæ‚¨çš„åŸå§‹å½±ç‰‡å°ºå¯¸ (1920x1080)ï¼Œæˆ‘ç‚ºæ‚¨è¦åŠƒäº†ä»¥ä¸‹æŠ•æ”¾ç­–ç•¥ï¼š\\n\\n**ã€ç­–ç•¥ä¸€ï¼šåŸå°ºå¯¸ç›´æ¥æŠ•æ”¾ã€‘**\\n*   **æ¨è–¦å°ºå¯¸ï¼šæ¨™æº–16:9 (1920x1080)**\\n    *   **æœ€ä½³ç‰ˆä½**ï¼šé€™èˆ‡æ‚¨çš„å½±ç‰‡å°ºå¯¸å®Œç¾ç›¸ç¬¦ï¼Œç„¡éœ€ä»»ä½•ä¿®æ”¹å³å¯ç›´æ¥æŠ•æ”¾æ–¼ **ä¿¡ç¾©å€å•†åœˆçš„ LED å¤§è¢å¹•** æˆ– **äº¤é€šæ¨ç´çš„æ©«å¹…è¢å¹•**ã€‚é€™æ¨£å¯ä»¥å®Œæ•´ä¿ç•™å½±ç‰‡çš„è¦–è¦ºè¡æ“ŠåŠ›ï¼Œæœ€å¤§åŒ–å»£å‘Šæ•ˆç›Šã€‚\\n\\n**ã€ç­–ç•¥äºŒï¼šæ™ºæ…§è£åˆ‡æŠ•æ”¾ã€‘**\\n*   **æ¨è–¦å°ºå¯¸ï¼šè±å±9:16 (1080x1920)**\\n    *   **æœ€ä½³ç‰ˆä½**ï¼šé›–ç„¶é€™éœ€è¦é€²è¡Œæ™ºæ…§è£åˆ‡ï¼Œä½†å°‡è¦–è¦ºç„¦é»é–å®šåœ¨å¥”è·‘çš„äººç‰©ä¸Šï¼Œéå¸¸é©åˆæŠ•æ”¾åœ¨ **æ·é‹ç«™æœˆå°çš„ç›´å¼è¢å¹•** æˆ– **é›»æ¢¯å…§çš„å»£å‘Šè¢å¹•**ï¼Œèƒ½åœ¨è¿‘è·é›¢å¸å¼•ä¹˜å®¢ç›®å…‰ã€‚",
  "recommended_template_names": ["æ¨™æº–16:9", "è±å±9:16"],
  "analysis_options": [
    {{
      "subject": "å¥”è·‘ä¸­çš„äººç‰©",
      "importance": "high",
      "confidence": 0.95,
      "center": [960, 540],
      "thumbnail": "data:image/jpeg;base64,..."
    }}
  ]
}}

**è«‹åš´æ ¼éµå¾ªä»¥ä¸Š JSON æ ¼å¼é€²è¡Œè¼¸å‡ºã€‚**
"""
        }
    ]

    # å°‡åœ–åƒå¹€æ·»åŠ åˆ°ç¬¬ä¸€æ¢ç”¨æˆ¶æ¶ˆæ¯ï¼ˆå¦‚æœæ­·å²ç‚ºç©ºï¼‰æˆ–æœ€æ–°çš„ç”¨æˆ¶æ¶ˆæ¯
    user_content = []
    
    # é¦–å…ˆæ·»åŠ æ­·å²è¨˜éŒ„
    if conversation_history:
        messages.extend(conversation_history)
        # æ‰¾åˆ°æœ€å¾Œä¸€å€‹ç”¨æˆ¶æ¶ˆæ¯ä¾†é™„åŠ åœ–åƒ
        last_user_message = next((msg for msg in reversed(messages) if msg['role'] == 'user'), None)
        if last_user_message:
            # å¦‚æœæ˜¯å­—ä¸²ï¼Œè½‰æ›ç‚ºåˆ—è¡¨
            if isinstance(last_user_message['content'], str):
                last_user_message['content'] = [{"type": "text", "text": last_user_message['content']}]
            # é™„åŠ åœ–åƒ
            for frame in base64_frames:
                last_user_message['content'].append({"type": "image_url", "image_url": {"url": f"data:image/jpeg;base64,{frame}"}})
        else:
            # å¦‚æœæ­·å²ä¸­æ²’æœ‰ç”¨æˆ¶æ¶ˆæ¯ï¼Œå‰µå»ºä¸€å€‹æ–°çš„
            messages.append({"role": "user", "content": [{"type": "text", "text": "è«‹åˆ†æé€™äº›ç•«é¢ã€‚"}] + [{"type": "image_url", "image_url": {"url": f"data:image/jpeg;base64,{frame}"}} for frame in base64_frames]})
    else:
        # å¦‚æœæ²’æœ‰æ­·å²è¨˜éŒ„ï¼Œå‰µå»ºåˆå§‹ç”¨æˆ¶æ¶ˆæ¯
        user_content.append({"type": "text", "text": "é€™æ˜¯è¦åˆ†æçš„å½±ç‰‡ï¼Œè«‹æä¾›åˆå§‹å°ˆæ¥­å»ºè­°ã€‚"})
        for frame in base64_frames:
            user_content.append({"type": "image_url", "image_url": {"url": f"data:image/jpeg;base64,{frame}"}})
        messages.append({"role": "user", "content": user_content})

    try:
        response = client.chat.completions.create(
            model="gpt-4o",
            messages=messages,
            response_format={"type": "json_object"},
            temperature=0.3,
        )
        response_content = response.choices[0].message.content
        print("âœ… LLMåˆ†ææˆåŠŸ")
        return json.loads(response_content)
    except Exception as e:
        print(f"âŒ LLMåˆ†æå¤±æ•—: {e}")
        return None

def extract_frames_from_video(video_path, max_frames=5, attempt=1):
    """å¾å½±ç‰‡ä¸­æå–å¹€ï¼Œæ”¯æ´å¤šæ¬¡å˜—è©¦ä»¥ç²å¾—ä¸åŒçš„å¹€"""
    cap = cv2.VideoCapture(video_path)
    if not cap.isOpened():
        return []

    base64_frames = []
    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
    fps = cap.get(cv2.CAP_PROP_FPS)
    
    if total_frames <= 0:
        cap.release()
        return []
    
    # æ ¹æ“šå˜—è©¦æ¬¡æ•¸èª¿æ•´å¹€æå–ç­–ç•¥
    if attempt == 1:
        # ç¬¬ä¸€æ¬¡å˜—è©¦ï¼šå‡å‹»åˆ†ä½ˆ
        frame_interval = max(1, total_frames // max_frames)
        start_offset = 0
    elif attempt == 2:
        # ç¬¬äºŒæ¬¡å˜—è©¦ï¼šå¾å½±ç‰‡ä¸­é–“é–‹å§‹
        frame_interval = max(1, total_frames // (max_frames * 2))
        start_offset = total_frames // 4
    else:
        # ç¬¬ä¸‰æ¬¡å˜—è©¦ï¼šéš¨æ©Ÿé¸æ“‡å¹€
        import random
        frame_indices = random.sample(range(total_frames), min(max_frames, total_frames))
        frame_indices.sort()
        
        for frame_idx in frame_indices:
            cap.set(cv2.CAP_PROP_POS_FRAMES, frame_idx)
            ret, frame = cap.read()
            if ret:
                _, buffer = cv2.imencode(".jpg", frame, [cv2.IMWRITE_JPEG_QUALITY, config.DEFAULT_JPEG_QUALITY])
                base64_frames.append(base64.b64encode(buffer).decode("utf-8"))
        
        cap.release()
        return base64_frames
    
    # æ¨™æº–æå–é‚è¼¯ï¼ˆç¬¬ä¸€æ¬¡å’Œç¬¬äºŒæ¬¡å˜—è©¦ï¼‰
    count = 0
    for i in range(start_offset, total_frames, frame_interval):
        if count >= max_frames:
            break
        cap.set(cv2.CAP_PROP_POS_FRAMES, i)
        ret, frame = cap.read()
        if ret:
            _, buffer = cv2.imencode(".jpg", frame, [cv2.IMWRITE_JPEG_QUALITY, config.DEFAULT_JPEG_QUALITY])
            base64_frames.append(base64.b64encode(buffer).decode("utf-8"))
            count += 1
    
    cap.release()
    return base64_frames

def crop_frame_for_thumbnail(video_path, box):
    """å¾å½±ç‰‡ä¸­é–“å¹€è£åˆ‡ä¸€å€‹å€åŸŸä½œç‚ºç¸®åœ–"""
    try:
        cap = cv2.VideoCapture(video_path)
        if not cap.isOpened():
            return None
        
        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        # Seek to middle frame, which is usually representative
        cap.set(cv2.CAP_PROP_POS_FRAMES, total_frames // 2)
        ret, frame = cap.read()
        cap.release()
        
        if not ret:
            return None
        
        # é€²è¡Œè£åˆ‡
        x1, y1, x2, y2 = box
        cropped_frame = frame[y1:y2, x1:x2]
        
        # æª¢æŸ¥è£åˆ‡å¾Œçš„å½±æ ¼æ˜¯å¦æœ‰æ•ˆ
        if cropped_frame.size == 0:
            print(f"âš ï¸ è£åˆ‡å€åŸŸç„¡æ•ˆ: {box}, å°è‡´ç¸®åœ–ç”Ÿæˆå¤±æ•—ã€‚")
            return None

        _, buffer = cv2.imencode(".jpg", cropped_frame, [cv2.IMWRITE_JPEG_QUALITY, 80])
        return f"data:image/jpeg;base64,{base64.b64encode(buffer).decode('utf-8')}"
    except Exception as e:
        print(f"âŒ ç‚ºç¸®åœ–è£åˆ‡å¹€æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
        return None

def perform_video_conversion(input_path, output_path, target_width, target_height, crop_mode='center', manual_center=None):
    """æ ¸å¿ƒè½‰æ›å‡½å¼"""
    try:
        if not MOVIEPY_AVAILABLE:
            print("MoviePy ä¸å¯ç”¨ï¼ŒåŸ·è¡Œæª”æ¡ˆè¤‡è£½ã€‚")
            shutil.copy2(input_path, output_path)
            return

        print(f"â–¶ï¸ MoviePy: é–‹å§‹è½‰æ›ï¼Œè¼¸å‡ºè‡³: {output_path}")
            
        with VideoFileClip(input_path) as clip:
            crop_center = (clip.w / 2, clip.h / 2)

            if manual_center:
                print(f"ğŸ§  ä½¿ç”¨æ‰‹å‹•é¸æ“‡çš„ä¸­å¿ƒé»: {manual_center}")
                crop_center = manual_center
            elif crop_mode == 'face':
                print("ğŸ§  MoviePy: å•Ÿç”¨AIäººè‡‰è¾¨è­˜...")
                ai_center = analyze_video_for_face_crop(input_path)
                if ai_center is not None:
                    crop_center = ai_center
            
            print("ğŸ“ MoviePy: è¨ˆç®—ç¸®æ”¾èˆ‡è£åˆ‡åƒæ•¸...")
            
            # ä½¿ç”¨å…±ç”¨çš„æ™ºæ…§è£åˆ‡é‚è¼¯
            # å…ˆå¾å½±ç‰‡ä¸­æå–ä¸€å¹€ä¾†è¨ˆç®—è£åˆ‡åƒæ•¸
            cap = cv2.VideoCapture(input_path)
            cap.set(cv2.CAP_PROP_POS_FRAMES, 0)
            ret, frame = cap.read()
            cap.release()
            
            if ret:
                # ä½¿ç”¨å…±ç”¨è£åˆ‡å‡½æ•¸è¨ˆç®—åƒæ•¸
                _, is_adjusted = apply_smart_crop(
                    frame, target_width, target_height, crop_center,
                    original_width=clip.w, original_height=clip.h
                )
                
                # è¨ˆç®—MoviePyéœ€è¦çš„è£åˆ‡åƒæ•¸
                scale = max(target_width / clip.w, target_height / clip.h)
                resized_clip = clip.resize(scale)
                
                # é‡æ–°è¨ˆç®—è£åˆ‡ä¸­å¿ƒé»ï¼ˆèˆ‡apply_smart_cropç›¸åŒçš„é‚è¼¯ï¼‰
                desired_center_x = crop_center[0] * scale
                desired_center_y = crop_center[1] * scale
                
                half_w = target_width / 2
                half_h = target_height / 2
                min_x = half_w
                max_x = resized_clip.w - half_w
                min_y = half_h
                max_y = resized_clip.h - half_h
                
                final_crop_x = max(min_x, min(desired_center_x, max_x))
                final_crop_y = max(min_y, min(desired_center_y, max_y))
                
                if is_adjusted:
                    print(f"âš ï¸ è£åˆ‡ä¸­å¿ƒé»å·²èª¿æ•´ä»¥é¿å…è¶…å‡ºé‚Šç•Œã€‚")
                    print(f"   åŸå§‹ä¸­å¿ƒ: ({desired_center_x:.0f}, {desired_center_y:.0f}) -> èª¿æ•´å¾Œ: ({final_crop_x:.0f}, {final_crop_y:.0f})")
                
                final_clip = resized_clip.crop(
                    x_center=final_crop_x, y_center=final_crop_y,
                    width=target_width, height=target_height
                )
            else:
                # å¦‚æœç„¡æ³•è®€å–å¹€ï¼Œä½¿ç”¨åŸå§‹é‚è¼¯
                scale = max(target_width / clip.w, target_height / clip.h)
                resized_clip = clip.resize(scale)
                final_clip = resized_clip.crop(
                    x_center=resized_clip.w / 2, y_center=resized_clip.h / 2,
                    width=target_width, height=target_height
                )
            
            temp_audio_filename = f"temp-audio-{uuid.uuid4()}.m4a"
            temp_audio_path = os.path.join(os.path.dirname(output_path), temp_audio_filename)

            print(f"âœï¸ MoviePy: é–‹å§‹å¯«å…¥è¼¸å‡ºæª”æ¡ˆè‡³ {output_path}")
            final_clip.write_videofile(
                str(output_path),
                codec='libx264', audio_codec='aac',
                temp_audiofile=str(temp_audio_path),
                remove_temp=True,
                verbose=False,
                logger=None
            )
            print("âœ… MoviePy: æª”æ¡ˆå¯«å…¥å®Œæˆã€‚")
            
            final_clip.close()
            resized_clip.close()
            
    except Exception as e:
        import traceback
        print(f"â€¼ï¸â€¼ï¸ MoviePy è½‰æ›æ ¸å¿ƒç™¼ç”Ÿè‡´å‘½éŒ¯èª¤ â€¼ï¸â€¼ï¸")
        print(traceback.format_exc())

    print("â³ ç­‰å¾…æ–‡ä»¶ç³»çµ±åŒæ­¥...")
    time.sleep(1)